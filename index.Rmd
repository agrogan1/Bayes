---
title: 'Bayesian Calculations: Simulation of Coin Flipping'
author: "Andy Grogan-Kaylor"
date: "`r format(Sys.Date(), format='%B %d, %Y')`"
output:
  html_document:
    code_folding: hide
    fig_height: 3
    highlight: haddock
    keep_md: no
    number_sections: yes
    theme: yeti
    css: styles.css
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
  tufte::tufte_handout:
    number_sections: yes
    toc: yes
    highlight: haddock
subtitle: Prior With 3 Values; Data on Coin Flips; Likelihood and Posterior
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE,
                      fig.fullwidth = TRUE,
                      fig.margin = TRUE)

library(pander)

library(tufte)

library(tibble)

```

```{r, eval = FALSE, echo = FALSE}

# P(A|B) = [P(B|A)P(A) / P(B)]
# P(H|D) = [P(D|H)P(H) / P(D)]

# posterior is proportional to prior times likelihood

```

# Background

Bayes Theorem allows us to state our *prior* beliefs, to calculate the *likelihood* of our data given those beliefs, and then to update those beliefs with data, thus arriving at a set of *posterior* beliefs.  However, Bayesian calculations can be difficult to understand. This document attempts to provide a simple walkthrough of some Bayesian calculations.

# Bayes Rule

> A draft interactive version of these ideas can be found [here](https://agrogan.shinyapps.io/shinyBayes2/).

Mathematically Bayes Theorem is as follows:

$$P(H|D) = \frac{P(D|H)P(H)}{P(D)}$$ 

In words, Bayes Theorem may be written as follows:

$$posterior = \frac{likelihood * prior}{data}$$

> Our posterior beliefs are proportional to our prior beliefs, multiplied by the likelihood of those beliefs, given the data.

# This Example

In this example, we provide an example of using Bayes Theorem to examine our conclusions about the proportion of heads when a coin is flipped 10 times. 

Conventionally, we call this proportion that we are trying to estimate $\theta$.

For the sake of simplicity, this example uses a relatively simple set of prior beliefs about 3 possible values for the proportion $\theta$.

> R code in this example is adapted and simplified  from Kruschke (2011), p. 70

# Prior

We set a simple set of prior beliefs, concerning 3 values of $\theta$, the proportion of heads.

```{r}

theta1 <- c(0.25, 0.50, 0.75) # candidate parameter values

ptheta1 <- c(.25, .50, .25) # prior probabilities

ptheta1 <- ptheta1/sum(ptheta1) # normalize

```

Our values of $\theta$ are `r pander(theta1)`, with probabilities $P(\theta)$ of `r pander(ptheta1)`.

```{r}

barplot(names.arg = theta1, 
        height = ptheta1,
        main = "prior probabilities",
        col = "#FFBB00") # graph

```

```{r}

myBayesianEstimates <- tibble(theta1, ptheta1)

pander(myBayesianEstimates) # nice table

```

# The Data

10 coin flips. 1 Heads. 9 Tails.

```{r}

data1 <- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0) # the data

data1_factor <- factor(data1,
                       levels = c(0,1),
                       labels = c("T", "H"))

```

```{r}

n_heads <- sum(data1 == 1) # number of heads

n_tails <- sum(data1 == 0) # number of tails

```

```{r, fig.fullwidth = FALSE, fig.margin = TRUE}

plot(x = seq(1,10), # x from 1 to 10
     y = rep(1,10), # y = 1
     col = data1_factor, # color by data 
     cex = 3, # size
     lwd = 2, # line width
     axes = FALSE,
     main = "10 Coin Tosses",
     xlab = " ",
     ylab = " ") 

text(x = seq(1,10), # x from 1 to 10
     y = rep(1,10), # y = 1
     labels = data1_factor) # label with data1_factor

```

# Likelihood

The likelihood is the probability that a given value of $\theta$ would produce this number of heads.

The probability of multiple independent events $A$, $B$, $C$, etc. is $P(A,B,C, ...) = P(A) * P(B) * P(C) * ...$.

Therefore, in this case, the likelihood is $[P(heads)]^{\text{number of heads}}$ and multiply this by $[P(tails)]^{\text{number of tails}}$. 

Thus:

$$\mathcal{L}(\theta) = \theta^{\text{number of heads}} * (1-\theta)^{\text{number of tails}}$$

```{r}

likelihood1 <- theta1^n_heads * (1 - theta1)^n_tails # likelihood

barplot(names.arg = theta1, 
        height = likelihood1,
        main = "likelihood",
        col = "#375E97") # graph

```

At this point our estimates include not only a value of $\theta$ and $P(\theta)$, but also the likelihood, $\mathcal{L}(\theta)$.

```{r}

myBayesianEstimates <- tibble(theta1, ptheta1, likelihood1)

pander(myBayesianEstimates) # nice table

```

# Posterior

We then calculate the denominator of Bayes theorem:  

$$\Sigma [\mathcal{L}(\theta) * P(\theta)]$$

```{r}

pdata1 <- sum(likelihood1 * ptheta1) # normalize

```

We then use Bayes Rule to calculate the posterior:

$$P(H|D) = \frac{P(D|H)P(H)}{P(D)}$$ 

```{r}

posterior1 <- likelihood1 * ptheta1 / pdata1 # Bayes Rule

barplot(names.arg = theta1, 
        height = posterior1,
        main = "posterior",
        col = "#3F681C") # graph


```

Our estimates now include $\theta$, $P(\theta)$, $\mathcal{L}(\theta)$ and $P(\theta | D)$.

```{r}

myBayesianEstimates <- tibble(theta1, ptheta1, likelihood1, posterior1)

pander(myBayesianEstimates) # nice table

```

# Credits

Prepared by Andy Grogan-Kaylor [agrogan@umich.edu](agrogan@umich.edu), [www.umich.edu/~agrogan](www.umich.edu/~agrogan).

Questions, comments and corrections are most welcome.



