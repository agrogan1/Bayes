---
title: 'Bayesian Calculations: Simulation of Coin Flipping'
author: "Andy Grogan-Kaylor"
date: "`r format(Sys.Date(), format='%B %d, %Y')`"
output:
  html_document:
    fig_height: 3
    highlight: haddock
    keep_md: no
    number_sections: yes
    theme: yeti
    css: styles.css
    code_folding: hide
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
  tint::tintHtml:
    number_sections: yes
    toc: yes
    fig_height: 3
    highlight: haddock
  revealjs::revealjs_presentation:
    fig_height: 4
    highlight: haddock
    reveal_options:
      chalkboard:
        theme: chalkboard
        toggleChalkboardButton: yes
        toggleNotesButton: yes
      previewLinks: yes
      progress: yes
      slideNumber: yes
    reveal_plugins:
    - chalkboard
    - zoom
    self_contained: no
    theme: default
    transition: convex
  slidy_presentation: default
  ioslides_presentation:
    smaller: yes
subtitle: Prior With 3 Values; Data on Coin Flips; Likelihood and Posterior
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE,
                      fig.fullwidth = TRUE)

library(pander)

```

```{r, eval = FALSE, echo = FALSE}

# P(A|B) = [P(B|A)P(A) / P(B)]
# P(H|D) = [P(D|H)P(H) / P(D)]

# posterior is proportional to prior times likelihood

```

# Bayes Rule

Mathematically Bayes Theorem is as follows:

$$P(H|D) = \frac{P(D|H)P(H)}{P(D)}$$ 

In words, Bayes Theorem may be written as follows:

$$posterior = \frac{likelihood * prior}{data}$$

> Our posterior beliefs are proportional to our prior beliefs, multiplied by the likelihood of those beliefs given the data.

More colloquially, perhaps, Bayes Theorem allows us to state our prior beliefs, and then to update those beliefs with data, thus arriving at an updated set of beliefs.

# This Example

In this example, we provide an example of using Bayes Theorem to examine our conclusions about the proportion of heads when a coin is flipped 10 times. 

Conventionally, we call this proportion that we are trying to estimate $\theta$.

For the sake of simplicity, this example uses a relatively simple set of prior beliefs about 3 possible values for the proportion $\theta$.

> R code in this example is adapted and simplified  from Kruschke (2011), p. 70

# Prior

We set a simple set of prior beliefs, concerning 3 values of $\theta$, the proportion of heads.

```{r}

theta1 <- c(0.25, 0.50, 0.75) # candidate parameter values

ptheta1 <- c(.25, .50, .25) # prior probabilities

ptheta1 <- ptheta1/sum(ptheta1) # normalize

```

Our values of $\theta$ are `r pander(theta1)`, with probabilities $P(\theta)$ of `r pander(ptheta1)`.

```{r}

barplot(names.arg = theta1, 
        height = ptheta1,
        main = "prior probabilities",
        col = "#FFBB00") # graph

```

```{r}

myBayesianEstimates <- data.frame(theta1, ptheta1)

pander(myBayesianEstimates) # nice table

```

# The Data

10 coin flips. 1 Heads. 9 Tails.

```{r}

data1 <- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0) # the data

data1_factor <- factor(data1,
                       levels = c(0,1),
                       labels = c("T", "H"))

```

```{r}

n_heads <- sum(data1 == 1) # number of heads

n_tails <- sum(data1 == 0) # number of tails

```

```{r}

plot(x = seq(1,10), # x from 1 to 10
     y = rep(1,10), # y = 1
     col = data1_factor, # color by data 
     cex = 5, # size
     lwd = 2, # line width
     axes=FALSE,
     main = "10 Coin Tosses",
     xlab = " ",
     ylab = " ") 

text(x = seq(1,10), # x from 1 to 10
     y = rep(1,10), # y = 1
     labels = data1_factor) # label with data1_factor

```

# Likelihood

The likelihood is the likelihood that a given value of $\theta$ would produce this number of heads.

Since the probability of two independent events $A$ and $B$ is $P(A,B) = P(A) * P(B)$, we need to calculate $[P(heads)]^{\text{number of heads}}$ and multiply this by $[P(tails)]^{\text{number of tails}}$. 

Thus:

$$\mathcal{L}(\theta) = \theta^{\text{number of heads}} * (1-\theta)^{\text{number of tails}}$$

```{r}

likelihood1 <- theta1^n_heads * (1 - theta1)^n_tails # likelihood

barplot(names.arg = theta1, 
        height = likelihood1,
        main = "likelihood",
        col = "#375E97") # graph

```

At this point our estimates include not only a value of $\theta$ and $P(\theta)$, but also the likelihood, $\mathcal{L}(\theta)$.

```{r}

myBayesianEstimates <- data.frame(theta1, ptheta1, likelihood1)

pander(myBayesianEstimates) # nice table

```

# Posterior

We then calculate the denominator of Bayes theorem:  

$$\Sigma [\mathcal{L}(\theta) * P(\theta)]$$

```{r}

pdata1 <- sum(likelihood1 * ptheta1) # normalize

```

We then use Bayes Rule to calculate the posterior:

$$P(H|D) = \frac{P(D|H)P(H)}{P(D)}$$ 

```{r}

posterior1 <- likelihood1 * ptheta1 / pdata1 # Bayes Rule

barplot(names.arg = theta1, 
        height = posterior1,
        main = "posterior",
        col = "#3F681C") # graph


```

Our estimates now include $\theta$, $P(\theta)$, $\mathcal{L}(\theta)$ and $P(\theta | D)$.

```{r}

myBayesianEstimates <- data.frame(theta1, ptheta1, likelihood1, posterior1)

pander(myBayesianEstimates) # nice table

```





