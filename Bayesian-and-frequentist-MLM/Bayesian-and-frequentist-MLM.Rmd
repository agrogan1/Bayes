---
title: "Bayesian and Frequentist Multilevel Modeling"
author: "Andy Grogan-Kaylor"
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    highlight: haddock
    toc: yes
  tufte::tufte_handout:
    highlight: haddock
    number_sections: yes
    toc: yes
  html_document:
    css: UM.css
    fig_height: 3
    highlight: haddock
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
  pdf_document: 
    fig_caption: yes
    fig_height: 3
    number_sections: yes
    toc: yes
  tufte::tufte_html:
    highlight: haddock
    number_sections: yes
    toc: yes
header-includes: \usepackage{draftwatermark}
link-citations: yes
bibliography: Bayesian-and-frequentist-MLM.bib
biblio-style: apalike
citation_url: https://agrogan1.github.io/Bayes/Bayesian-and-frequentist-MLM/Bayesian-and-frequentist-MLM.html
---

```{r, echo=FALSE, eval=FALSE}

# Bayes may perform better (see cites in MBOX)

# weak ICC
# random slopes close to 0 
# non normal parameters, both \beta and \u's
# Small N *especially at Level 2*
# Stunnenberg et al. (2018) looked at the 
# probability that the effect would be 
# clinically meaningful for a particular patient.

```


\SetWatermarkText{ROUGH DRAFT}
\SetWatermarkFontSize{3cm}

```{r setup, include=FALSE}

library(tint)

# invalidate cache when the package version changes

knitr::opts_chunk$set(tidy = FALSE, 
                      # cache.extra = packageVersion('tint'),
                      echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.margin = TRUE)

options(htmltools.dir.version = FALSE)

library(MASS)

library(ggplot2)

library(ggthemes)

```

# Introduction

$$y_{ij} = \beta_0 + \beta_1 x_{1i} + u_{0j} + e_{ij}$$

All multilevel models account for group structure, in estimating the association of $x$ and $y$, by including a random intercept ($u_0$), and possibly one or more random slope terms ($u_1, u_2, etc....$).

Bayesian models may offer some advantages over frequentist models, but may be *substantially* slower to converge.

# Conceptual Appropriateness

Following [@Kruschke2014] all Bayesian models have a *conceptual appropriateness*. 

In frequentist reasoning we are estimating the probability of observing data at least as extreme as our data, while assuming a null hypothesis ($H_0$). Quite often, $H_0$, *e.g.* $\beta = 0$, or $\bar{x}_A - \bar{x}_B = 0$, is not a substantively interesting or substantively meaningful hypothesis. 

# Accepting the Null Hypothesis ($H_0$)

In Bayesian analysis, we are not rejecting a null hypothesis. Instead, we are *directly estimating the value of a parameter* such as $\beta$ and are indeed estimating a *full probability distribution* for this parameter.

Such an approach means that we have the ability to accept the null hypothesis $H_0$ [@Kruschke2018]. This ability to accept $H_0$ might possibly lead to theory simplification [@Morey2018], as well as to a lower likelihood of the publication bias that results from frequentist methods predicated upon the rejection of $H_0$ [@Kruschke2018].

# Prior Information

Bayesian models allow one to incorporate prior information about a parameter of interest.

Prior information may come from the prior research literature, e.g. from systematic reviews or meta-analyses, or expert opinion or clinical wisdom.

# Smaller Samples

Bayesian multilevel models may be better with small samples, especially samples with small numbers of Level 2 units [@Hox2012]. It is not clear to what degree this improvement in performance is dependent upon the use of informative priors.

# Full Distribution of Parameters

Bayesian models of all kinds provide full distributions of the parameters (e.g. $\beta$'s and random effects ($u$'s))--both singly and jointly--rather than only point estimates.

Information about the full distribution of a parameter, such as the estimate of the probability distribution of values of a risk factor, a protective factor, or the effect of an intervention, may be substantively meaningful. Such information may be especially important when the distribution of a regression parameter is non-normal [@VandeSchoot2014].

```{r, fig.height=3, fig.cap="Distribution of a Single Parameter"}

beta1 <- rbeta(10000, 1.0, 2.5)

ggplot(data = NULL,
       aes(x = beta1)) + 
  geom_density(fill="blue", alpha = .25) +
  geom_vline(xintercept = median(beta1)) +
  geom_text(aes(x=median(beta1), 
                label="\nmedian", y = 1), 
            colour="red", 
            angle= -90) +
  theme_tufte() + 
  labs(title = "Distribution of \u03b2",
       subtitle = "\u03b2 is Skewed",
       x = "\u03b2") 

```

```{r, fig.height=3, fig.cap="Joint Distribution of Parameters"}

# inspired by a coding approach at
# https://stackoverflow.com/questions/11530010/how-to-simulate-bimodal-distribution

# u0

u0 <- rnorm(1000, 0.5, 2)

# u1

u1a <- rnorm(1000, 0, .1)

u1b <- rnorm(1000, 1, .1)

flag <- rbinom(1000, size=1, prob=.5)

u1 <- u1a * (1 - flag) + u1b * flag 

# density plot

mydensity <- kde2d(u0, u1)

par(mar = rep(2, 4)) # adjust margins

persp(mydensity,
      theta = -45,
      phi = 10,
      main = "Joint Distribution of Random Effects",
      xlab = "random intercept",
      ylab = "random slope",
      zlab = "probability",
      lwd = .5)

# pairs(data.frame(beta1,beta2))

# library(GGally)
# 
# ggpairs(data.frame(beta1,beta2),
#         lower = list(continuous = "density"))

```

As Stata Corporation notes, "In a Bayesian multilevel model, *random effects* are model parameters just like regression coefficients and variance components" [@StataCorp2020].  This ability to estimate the *distributions* of these random effects means that the *distribution* of the random effect for one group can be compared to another. For example, the *distribution* of a parameter in one country could be directly compared to the *distribution* of that same parameter in another country. One could even estimate the probability that a particular $\beta$ had a higher value in one group (e.g. country), than in another. As Balov [-@Balov2016] suggests, this Bayesian approach allows us to "quantify the credibility" of these comparisons, which would not be possible with a frequentist approach.

As an example, Stunnenberg et al. [-@Stunnenberg2018] conducted a Bayesian analysis where the results of the multilevel analysis were used to inform treatment decisions. Here the data were repeated measures on patients, and thus the patients were the groups: "On completion of each treatment set, a Bayesian analysis was conducted to calculate the posterior probability of mexiletine [treatment] producing a clinically meaningful difference in the individual patient." 

# Distributional Models

Bayesian estimators allow one to directly model $\sigma_{u_0}$, the variance of the Level 2 units, as a function of covariates [@Burkner2018].

# Non-Linear Terms

```{r, fig.cap="Non-Linear Terms"}

# options(scipen = 999)

x <- rnorm(100, 0, 10)

e <- rnorm(100, 0, 1000)

y <- x^3 + e

ggplot(data=NULL,
       aes(x = x,
           y = y)) + 
  geom_point() + 
  geom_smooth() +
  labs(title = "Non-Linear Smoother") +
  # theme_minimal()
  theme_tufte()

```

Bayesian estimators allow for the incorporation of non-linear terms [@Burkner2018].[^overfitting]

[^overfitting]: Such non-linear terms offer ways of non-parametrically fitting curvature. Do they represent over-fitting? Do they provide substantively interpretable results?

# Maximal Models

Bayesian estimators allow for the estimation of so called *maximal models* [@Barr2013; @Frank2018], which allow for the inclusion of a large number of random slopes, e.g. $u_1, u_2, u_3, ..., etc.$ even when some of those estimated slopes are close to 0. 

In contrast, [@Matuschek2017] argue that such a *maximal* approach may lead to a loss of statistical power and further argue that one should adhere to "a random effect structure that is supported by the data."

# References









