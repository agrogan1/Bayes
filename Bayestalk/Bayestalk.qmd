---
title: "A Discussion Focused Approach to Bayesian Statistics"
title-slide-attributes:  
  data-background-color: black
author: "Andy Grogan-Kaylor"
date: "today"
format: 
  revealjs:
    scrollable: true
    transition: convex
    controls: true
    controls-tutorial: true
    slide-number: true
    chalkboard: true
    logo: Thomas_Bayes.png
execute: 
  freeze: auto
---

# Navigation 

* **o** for outline
* **f** for full screen
* **Alt-Click** to zoom
* Scroll &#8681; to the bottom of each column of slides; then &#8680;

# Beginning

## Background

::: {layout="[1,1]" layout-valign="bottom"}
![Thomas Bayes](Thomas_Bayes.png)

![Pierre-Simon Laplace](Laplace_Pierre-Simon.png)
:::

Bayesian statistics are often touted as the *new statistics*, that should perhaps supplant traditional *frequentist* statistics. 

## Advantages

Bayesian statistics have the advantage of being able to incorporate prior information (e.g. clinical wisdom & insight, community knowledge, results of meta-analyses and literature reviews). 

Bayesian statistics also have the advantage that we are not simply deciding whether to reject the null hypotheses, $P(D|H_0)$ but are rather able to evaluate the probability of a hypothesis, $P(H|D)$. In essence, this means that we can sometimes decide to *accept* the null hypothesis, rather than *failing to reject the null hypothesis*.

## Disadvantages

However, Bayesian statistics may be difficult to understand. Further, while statistical software for Bayesian inference is becoming easier and easier to use, syntax for Bayesian inference may remain complicated and counter-intuitive 

Lastly, the advantages of being able to *incorporate prior information* and to *accept the null hypothesis* may be overstated: *prior information* may be difficult to come by; and social science may be able to move forward quite well without the ability to *accept null hypotheses*.

In this presentation I hope to at least introduce some of these ideas, and to generate some discussion of them. 

# Bayes Theorem As Equation & Words (10 Minutes)

## Equation

$$P(H|D) = \frac{P(D|H) P(H)}{P(D)}$$ 

## Words

$$\text{posterior} \propto \text{likelihood} \times \text{prior}$$ 

## In Even More Words:

Our posterior (updated) beliefs about the probability of some hypothesis are proportional to our prior beliefs about the probability of that hypothesis multiplied by the likelihood that were our hypothesis true it would have generated the data we observe.

# Why Do We Care / Should We Care? (40 Minutes Total)

## Prior Information (20 Minutes)

[https://agrogan.shinyapps.io/Thinking-Through-Bayes/](https://agrogan.shinyapps.io/Thinking-Through-Bayes/)

## Accepting The Null Hypothesis (20 Minutes) {.smaller}

In Bayesian analysis, we are *not* assessing the plausibility of the data ($P(D|H)$), given the assumption of a null hypothesis $H_0: \text{parameter value} = 0$. This feature of Bayesian analysis has a few key implications:

* We are actually estimating--*the probability of a particular hypothesis given the data*--what we often *think* we are estimating in Frequentist analysis. Thus, after conducting a Bayesian analysis, we can simply say, "The probability of our hypothesis ($P(H|D)$) given the data is *x*," rather than engaging in complicated statements about "Were the null hypothesis to be true, we estimate that it is *p* likely that we would see data as extreme, or more extreme, than we actually observed."
    
* Because we are directly estimating the probability of hypotheses, we can not only evaluate the probability of the null hypothesis ($H_0$), but also accept the null hypothesis ($H_0$), something that we are never supposed to be able to do in frequentist analysis. Being able to accept the null hypothesis may have implications for *equivalency testing* and *theory simplification* and may reduce publication bias, if we are not always looking for ways to reject $H_0$.
    
[https://agrogan1.github.io/Bayes/accepting-H0/accepting-H0.html](https://agrogan1.github.io/Bayes/accepting-H0/accepting-H0.html)
    
# Lab and Questions (Work At Your Own Pace and Leave When You're Done) (Last 60 Minutes)

Thinking through Bayes is *hard*. We perhaps learn best by doing.

## Our Data

We are using data from my text on multilevel modeling, *Multilevel Thinking*: [https://agrogan1.github.io/multilevel-thinking/simulated-multi-country-data.html](https://agrogan1.github.io/multilevel-thinking/simulated-multi-country-data.html).

We will be ignoring the fact that this data comes from multiple countries. 

```{r}

library(Statamarkdown)

```

```{stata, echo = TRUE, collectcode = TRUE}

use "https://github.com/agrogan1/multilevel-thinking/raw/main/simulate-and-analyze-multilevel-data/simulated_multilevel_data.dta"

```

## Basic Bayesian Regression {.smaller}

What do the results say (particularly about HDI)?

```{stata, echo=TRUE}

bayes: regress outcome physical_punishment warmth HDI

```

## Basic Bayesian Regression {.smaller}

Here we state our prior beliefs about the $\beta_{HDI} HDI$

We think it is 1, but put some uncertainty around that belief ($sd = 10$)

```{stata, echo=TRUE}

bayes, prior({outcome:HDI}, normal(1,10)): regress outcome physical_punishment warmth HDI

```





