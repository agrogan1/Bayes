---
title: 'Bayesian Calculations: Simulation of Coin Flipping'
author: "Andy Grogan-Kaylor"
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    highlight: haddock
    toc: yes
subtitle: 'Prior With 3 Values; Data on Coin Flips; Likelihood and Posterior'
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE,
                      fig.margin = TRUE)

library(pander)

library(tufte)

library(tibble)

library(ggplot2)

```

```{r, eval = FALSE, echo = FALSE}

# P(A|B) = [P(B|A)P(A) / P(B)]
# P(H|D) = [P(D|H)P(H) / P(D)]

# posterior is proportional to prior times likelihood

```

# Background

[Bayes Theorem](./Bayes-Theorem/Bayes-Theorem.html) allows us to state our *prior* beliefs, to calculate the *likelihood* of our data given those beliefs, and then to update those beliefs with data, thus arriving at a set of *posterior* beliefs.  However, Bayesian calculations can be difficult to understand. This document attempts to provide a simple walkthrough of some Bayesian calculations.

# Bayes Rule

Mathematically Bayes Theorem is as follows:

$$P(H|D) = \frac{P(D|H)P(H)}{P(D)}$$ 

In words, Bayes Theorem may be written as follows:

$$posterior = \frac{likelihood * prior}{data}$$

> Our posterior beliefs are proportional to our prior beliefs, multiplied by the likelihood of those beliefs, given the data.

# This Example

In this example, we provide an example of using Bayes Theorem to examine our conclusions about the proportion of heads when a coin is flipped 10 times. 

Conventionally, we call this proportion that we are trying to estimate $\theta$.

For the sake of simplicity, this example uses a relatively simple set of prior beliefs about 3 possible values for the proportion $\theta$.

> R code in this example is adapted and simplified  from Kruschke (2011), p. 70

# Prior

We set a simple set of prior beliefs, concerning 3 values of $\theta$, the proportion of heads.

```{r}

theta1 <- c(0.25, 0.50, 0.75) # candidate parameter values

ptheta1 <- c(.25, .50, .25) # prior probabilities

ptheta1 <- ptheta1/sum(ptheta1) # normalize

```

Our values of $\theta$ are `r pander(theta1)`, with probabilities $P(\theta)$ of `r pander(ptheta1)`.

```{r}

ggplot(data = NULL,
       aes(x = theta1,
           y = ptheta1)) +
  geom_bar(stat = "identity", 
           fill = "#FFBB00") +
  labs(title = "Prior Probabilities") +
  theme_minimal()

```

```{r}

myBayesianEstimates <- tibble(theta1, ptheta1)

pander(myBayesianEstimates) # nice table

```

# The Data

10 coin flips. 1 Heads. 9 Tails.

```{r}

data1 <- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0) # the data

data1_factor <- factor(data1,
                       levels = c(0,1),
                       labels = c("T", "H"))

```

```{r}

n_heads <- sum(data1 == 1) # number of heads

n_tails <- sum(data1 == 0) # number of tails

```

```{r, fig.height=2}


x <- seq(1,10)

y <- rep(1,10)

coindata <- data.frame(x, y, data1_factor)

ggplot(coindata,
       aes(x = x,
           y = y,
           label = data1_factor,
           color = data1_factor)) +
  geom_point(size = 10, shape = 1, pch=19) +
  geom_text() +
  labs(x = "",
       y = "") +
  scale_color_manual(values = c("black", "red")) +
  theme_void() +
  theme(legend.position = "none")


```

# Likelihood

The likelihood is the probability that a given value of $\theta$ would produce this number of heads.

The probability of multiple independent events $A$, $B$, $C$, etc. is $P(A,B,C, ...) = P(A) * P(B) * P(C) * ...$.

Therefore, in this case, the likelihood is proportional to $[P(heads)]^{\text{number of heads}}$ and multiply this by $[P(tails)]^{\text{number of tails}}$. 

Thus:

$$\mathcal{L}(\theta) \propto \theta^{\text{number of heads}} * (1-\theta)^{\text{number of tails}}$$

```{r}

likelihood1 <- theta1^n_heads * (1 - theta1)^n_tails # likelihood

ggplot(data = NULL,
       aes(x = theta1,
           y = likelihood1)) +
  geom_bar(stat = "identity", 
           fill = "#375E97") +
  labs(title = "Likelihood") +
  theme_minimal()

```

At this point our estimates include not only a value of $\theta$ and $P(\theta)$, but also the likelihood, $\mathcal{L}(\theta)$.

```{r}

myBayesianEstimates <- tibble(theta1, ptheta1, likelihood1)

pander(myBayesianEstimates) # nice table

```

# Posterior

We then calculate the denominator of Bayes theorem:  

$$\Sigma [\mathcal{L}(\theta) * P(\theta)]$$

```{r}

pdata1 <- sum(likelihood1 * ptheta1) # normalize

```

We then use Bayes Rule to calculate the posterior:

$$P(H|D) = \frac{P(D|H)P(H)}{P(D)}$$ 

```{r}

posterior1 <- likelihood1 * ptheta1 / pdata1 # Bayes Rule

ggplot(data = NULL,
       aes(x = theta1,
           y = posterior1)) +
  geom_bar(stat = "identity", 
           fill = "#3F681C") +
  labs(title = "Posterior") +
  theme_minimal()

```

Our estimates now include $\theta$, $P(\theta)$, $\mathcal{L}(\theta)$ and $P(\theta | D)$.

```{r}

myBayesianEstimates <- tibble(theta1, ptheta1, likelihood1, posterior1)

pander(myBayesianEstimates) # nice table

```

# Credits

Prepared by Andy Grogan-Kaylor [agrogan@umich.edu](agrogan@umich.edu), [www.umich.edu/~agrogan](www.umich.edu/~agrogan).

Questions, comments and corrections are most welcome.



